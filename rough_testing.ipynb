{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed98be43-aa4d-43ee-ae3d-2edfb2a8401f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kagglehub\n",
      "  Downloading kagglehub-0.3.12-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
      "Collecting tqdm (from kagglehub)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.1.31)\n",
      "Downloading kagglehub-0.3.12-py3-none-any.whl (67 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, kagglehub\n",
      "Successfully installed kagglehub-0.3.12 tqdm-4.67.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: iterative-stratification in /usr/local/lib/python3.11/dist-packages (0.1.9)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from iterative-stratification) (2.1.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from iterative-stratification) (1.16.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from iterative-stratification) (1.7.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->iterative-stratification) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->iterative-stratification) (3.6.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub\n",
    "!pip install iterative-stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe24c54a-de60-4cb8-9e5f-21f2941ed19e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kagglehub'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkagglehub\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Download latest version\u001b[39;00m\n\u001b[32m      4\u001b[39m path = kagglehub.dataset_download(\u001b[33m\"\u001b[39m\u001b[33mxhlulu/vinbigdata-chest-xray-resized-png-256x256\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'kagglehub'"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"xhlulu/vinbigdata-chest-xray-resized-png-256x256\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7babdcf8-a0c0-4e5c-ac24-4048820b0212",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv /root/.cache/kagglehub/datasets/xhlulu/vinbigdata-chest-xray-resized-png-256x256/versions/1 /workspace/256data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f8b23fe-9775-4e09-8d03-a1ad0317b335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93e64204-c71f-4220-a8b9-87d8aa2f5b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple, Optional\n",
    "\n",
    "NO_FINDING = 14\n",
    "\n",
    "def warp_box(box, sx, sy): #scale factor x, scale factor y\n",
    "    if box is None: return None\n",
    "    x1,y1,x2,y2 = box\n",
    "    return [x1*sx, y1*sy, x2*sx, y2*sy]\n",
    "\n",
    "def clamp_box(box, w, h):\n",
    "    if box is None: return None\n",
    "    x1,y1,x2,y2 = box\n",
    "    x1 = max(0.0, min(w, x1)); y1 = max(0.0, min(h, y1))\n",
    "    x2 = max(0.0, min(w, x2)); y2 = max(0.0, min(h, y2))\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return None\n",
    "    return [x1,y1,x2,y2]\n",
    "\n",
    "def transform_boxes_to_224(\n",
    "    df: pd.DataFrame,\n",
    "    mode: str = \"from_256\",  # or \"from_original\"\n",
    "    per_image_src_size: Optional[Dict[str, Tuple[int,int]]] = None,\n",
    "    class_no_finding: int = NO_FINDING,\n",
    "    drop_degenerate: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Expects columns: image_id, class_id, x_min, y_min, x_max, y_max\n",
    "    Returns a copy with boxes mapped to a 224x224 image via warp.\n",
    "    \"\"\"\n",
    "    req_cols = {\"image_id\",\"class_id\",\"x_min\",\"y_min\",\"x_max\",\"y_max\"}\n",
    "    missing = req_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "    out = df.copy()\n",
    "    w_target = h_target = 224\n",
    "    scale_256_to_224 = w_target / 256.0  # == 224/256\n",
    "\n",
    "    new_x1,new_y1,new_x2,new_y2,keep = [],[],[],[],[]\n",
    "\n",
    "    for row in out.itertuples(index=False):\n",
    "        img = getattr(row, \"image_id\")\n",
    "        cls = getattr(row, \"class_id\")\n",
    "        x1 = getattr(row, \"x_min\"); y1 = getattr(row, \"y_min\")\n",
    "        x2 = getattr(row, \"x_max\"); y2 = getattr(row, \"y_max\")\n",
    "\n",
    "        # Keep NaNs for No finding\n",
    "        if cls == class_no_finding or any(pd.isna(v) for v in (x1,y1,x2,y2)):\n",
    "            new_x1.append(np.nan); new_y1.append(np.nan)\n",
    "            new_x2.append(np.nan); new_y2.append(np.nan)\n",
    "            keep.append(True)\n",
    "            continue\n",
    "\n",
    "        box = [float(x1), float(y1), float(x2), float(y2)]\n",
    "\n",
    "        if mode == \"from_original\":\n",
    "            if per_image_src_size is None or img not in per_image_src_size:\n",
    "                raise ValueError(f\"Missing original size for image_id={img}\")\n",
    "            W0,H0 = per_image_src_size[img]\n",
    "            # warp original -> 256\n",
    "            sx1 = 256.0 / float(W0); sy1 = 256.0 / float(H0)\n",
    "            box = warp_box(box, sx1, sy1)\n",
    "            box = clamp_box(box, 256.0, 256.0)\n",
    "            if box is None:\n",
    "                if drop_degenerate: keep.append(False); continue\n",
    "                else:\n",
    "                    new_x1.append(np.nan); new_y1.append(np.nan)\n",
    "                    new_x2.append(np.nan); new_y2.append(np.nan)\n",
    "                    keep.append(True); continue\n",
    "\n",
    "        elif mode != \"from_256\":\n",
    "            raise ValueError(\"mode must be 'from_256' or 'from_original'\")\n",
    "\n",
    "        # warp 256 -> 224\n",
    "        box = warp_box(box, scale_256_to_224, scale_256_to_224)\n",
    "        box = clamp_box(box, w_target, h_target)\n",
    "        if box is None:\n",
    "            if drop_degenerate: keep.append(False); continue\n",
    "            else:\n",
    "                new_x1.append(np.nan); new_y1.append(np.nan)\n",
    "                new_x2.append(np.nan); new_y2.append(np.nan)\n",
    "                keep.append(True); continue\n",
    "\n",
    "        x1,y1,x2,y2 = box\n",
    "        new_x1.append(x1); new_y1.append(y1)\n",
    "        new_x2.append(x2); new_y2.append(y2)\n",
    "        keep.append(True)\n",
    "\n",
    "    out = out.loc[keep].copy()\n",
    "    out[\"x_min\"] = new_x1[:len(out)]\n",
    "    out[\"y_min\"] = new_y1[:len(out)]\n",
    "    out[\"x_max\"] = new_x2[:len(out)]\n",
    "    out[\"y_max\"] = new_y2[:len(out)]\n",
    "    return out\n",
    "\n",
    "# --- Examples ---\n",
    "\n",
    "# 1) Boxes already on 256 grid\n",
    "# df_224 = transform_boxes_to_224(df, mode=\"from_256\")\n",
    "\n",
    "# 2) Boxes in original DICOM coords; you must supply per-image (W0,H0):\n",
    "# per_image_src_size = dict(zip(meta_df.image_id, zip(meta_df.dim1, meta_df.dim0)))  # note: (W0,H0)\n",
    "# df_224 = transform_boxes_to_224(df, mode=\"from_original\", per_image_src_size=per_image_src_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fb34b4e-6d36-467a-a44d-d31baf37c592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 344 ms, sys: 51.5 ms, total: 396 ms\n",
      "Wall time: 401 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "meta_df = pd.read_csv('train.csv')\n",
    "per_image_src_size = dict(zip(meta_df.image_id, zip(meta_df.width, meta_df.height)))  # note: (W0,H0)\n",
    "df_224 = transform_boxes_to_224(meta_df, mode=\"from_original\", per_image_src_size=per_image_src_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf067ade-4956-48e9-8dfd-037ed6a7f342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               image_id          class_name  class_id rad_id  \\\n",
      "0      50a418190bc3fb1ef1633bf9678929b3          No finding        14    R11   \n",
      "1      21a10246a5ec7af151081d0cd6d65dc9          No finding        14     R7   \n",
      "2      9a5094b2563a1ef3ff50dc5c7ff71345        Cardiomegaly         3    R10   \n",
      "3      051132a778e61a86eb147c7c6f564dfe  Aortic enlargement         0    R10   \n",
      "4      063319de25ce7edb9b1c6b8881290140          No finding        14    R10   \n",
      "...                                 ...                 ...       ...    ...   \n",
      "67909  936fd5cff1c058d39817a08f58b72cae          No finding        14     R1   \n",
      "67910  ca7e72954550eeb610fe22bf0244b7fa          No finding        14     R1   \n",
      "67911  aa17d5312a0fb4a2939436abca7f9579          No finding        14     R8   \n",
      "67912  4b56bc6d22b192f075f13231419dfcc8        Cardiomegaly         3     R8   \n",
      "67913  5e272e3adbdaafb07a7e84a9e62b1a4c          No finding        14    R16   \n",
      "\n",
      "            x_min       y_min       x_max       y_max  width  height  \n",
      "0             NaN         NaN         NaN         NaN   2332    2580  \n",
      "1             NaN         NaN         NaN         NaN   2954    3159  \n",
      "2       74.415385  131.849315  178.015385  175.575342   2080    2336  \n",
      "3      122.888889   57.788889  156.625000   79.255556   2304    2880  \n",
      "4             NaN         NaN         NaN         NaN   2540    3072  \n",
      "...           ...         ...         ...         ...    ...     ...  \n",
      "67909         NaN         NaN         NaN         NaN   2444    3200  \n",
      "67910         NaN         NaN         NaN         NaN   1994    2430  \n",
      "67911         NaN         NaN         NaN         NaN   2048    2500  \n",
      "67912   87.756098  107.498039  191.219512  143.952941   1968    2040  \n",
      "67913         NaN         NaN         NaN         NaN   2048    2500  \n",
      "\n",
      "[67914 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_224)\n",
    "df_224.to_csv(\"df_224.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "940dcd06-b2c9-4caa-8e19-4833690453d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_vindr_like.py\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import unicodedata\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Try to use iterative stratification; fall back to a simple stratifier with a warning.\n",
    "try:\n",
    "    from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "    HAS_ITERSTRAT = True\n",
    "except Exception:\n",
    "    HAS_ITERSTRAT = False\n",
    "\n",
    "NO_FINDING_ID = 14  # dataset convention\n",
    "\n",
    "def nfc_norm(s: str) -> str:\n",
    "    # NFC normalize + lowercase + strip spaces; \"canonical\" filename normalization\n",
    "    return unicodedata.normalize(\"NFC\", s).strip().lower()\n",
    "\n",
    "def load_annotations(csv_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # minimal schema checks\n",
    "    need = {\"image_id\",\"class_id\",\"rad_id\",\"x_min\",\"y_min\",\"x_max\",\"y_max\"}\n",
    "    miss = need - set(df.columns)\n",
    "    if miss:\n",
    "        raise ValueError(f\"CSV missing columns: {miss}\")\n",
    "    # normalize IDs\n",
    "    df[\"image_id\"] = df[\"image_id\"].astype(str).map(nfc_norm)\n",
    "    df[\"rad_id\"] = df[\"rad_id\"].astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "def build_multilabel_targets(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Build per-image targets.\n",
    "\n",
    "    Majority rule per class c in [0..13]: positive if >=2 distinct raters annotated at least one bbox of class c.\n",
    "    Soft label per class: (#distinct-raters-positive)/3 in {0, 1/3, 2/3, 1}.\n",
    "\n",
    "    no_finding_majority: 1 if >=2 raters said No finding AND no other class has majority positive.\n",
    "    \"\"\"\n",
    "    # Deduplicate boxes per (image, class, rater) so a rater with multiple boxes for same class counts once\n",
    "    df_pos = df[df[\"class_id\"] != NO_FINDING_ID].copy()\n",
    "    df_pos = df_pos.dropna(subset=[\"x_min\",\"y_min\",\"x_max\",\"y_max\"])\n",
    "    df_pos[\"rad_int\"] = df_pos[\"rad_id\"].str[1:].astype(int) if df_pos[\"rad_id\"].str.startswith(\"R\").all() else df_pos[\"rad_id\"]\n",
    "\n",
    "    # Rater -> class presence\n",
    "    raters_per_class = (\n",
    "        df_pos.groupby([\"image_id\",\"class_id\"])[\"rad_id\"]\n",
    "        .nunique()\n",
    "        .rename(\"rater_count\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Initialize label tables\n",
    "    images = sorted(df[\"image_id\"].unique())\n",
    "    classes = list(range(0, NO_FINDING_ID))  # 0..13\n",
    "    idx = pd.Index(images, name=\"image_id\")\n",
    "    y_major = pd.DataFrame(0, index=idx, columns=classes, dtype=np.int8)\n",
    "    y_soft   = pd.DataFrame(0.0, index=idx, columns=classes, dtype=float)\n",
    "\n",
    "    # Fill per-class labels\n",
    "    for _, row in raters_per_class.iterrows():\n",
    "        img, cid, rc = row[\"image_id\"], int(row[\"class_id\"]), int(row[\"rater_count\"])\n",
    "        if cid in classes:\n",
    "            y_soft.at[img, cid] = min(1.0, rc / 3.0)  # in {1/3, 2/3, 1} (or 0 if absent)\n",
    "            y_major.at[img, cid] = 1 if rc >= 2 else max(y_major.at[img, cid], 0)\n",
    "\n",
    "    # Ensure zeros where absent\n",
    "    y_soft = y_soft.fillna(0.0)\n",
    "    y_major = y_major.fillna(0).astype(np.int8)\n",
    "\n",
    "    # No finding counts per image (how many raters said \"No finding\")\n",
    "    nf_counts = (\n",
    "        df[df[\"class_id\"] == NO_FINDING_ID]\n",
    "        .groupby(\"image_id\")[\"rad_id\"].nunique()\n",
    "        .reindex(idx, fill_value=0)\n",
    "    )\n",
    "    # no_finding majority is 1 only if ≥2 raters say No finding and NO other class has majority positive\n",
    "    no_find_major = ((nf_counts >= 2) & (y_major.sum(axis=1) == 0)).astype(np.int8)\n",
    "\n",
    "    # Auxiliary: number of (unique) boxes per image (any class, any rater)\n",
    "    n_boxes = (\n",
    "        df_pos.groupby(\"image_id\")[[\"x_min\",\"y_min\",\"x_max\",\"y_max\",\"rad_id\",\"class_id\"]]\n",
    "        .apply(lambda g: len(g))\n",
    "        .reindex(idx, fill_value=0)\n",
    "        .rename(\"n_boxes\")\n",
    "    )\n",
    "\n",
    "    targets = {\n",
    "        \"y_major\": y_major,         # (N, 14) ints 0/1\n",
    "        \"y_soft\": y_soft,           # (N, 14) floats in [0,1]\n",
    "        \"no_finding_major\": no_find_major,  # (N,) ints 0/1\n",
    "        \"n_boxes\": n_boxes,         # (N,) ints\n",
    "    }\n",
    "    return targets\n",
    "\n",
    "def make_strat_matrix(targets, rare_bins=True):\n",
    "    \"\"\"\n",
    "    Build a multilabel stratification matrix Z (N, K).\n",
    "\n",
    "    Columns include:\n",
    "      - 14 binary majority labels (classes 0,1,2,..,13)\n",
    "      - one 'no_finding' column\n",
    "      - optional buckets of n_boxes to keep annotation complexity roughly matched\n",
    "    \"\"\"\n",
    "    Z = targets[\"y_major\"].copy()\n",
    "    Z[\"no_finding\"] = targets[\"no_finding_major\"]\n",
    "\n",
    "    if rare_bins:\n",
    "        # Bucketize n_boxes (annotation complexity) as a weak feature\n",
    "        nb = targets[\"n_boxes\"].clip(upper=60)\n",
    "        # simple bins: 0-3, 4-9, 10-19, >=20\n",
    "        bins = pd.cut(nb, bins=[-1,3,9,19,1000], labels=[\"b0_3\",\"b4_9\",\"b10_19\",\"b20p\"])\n",
    "        for lab in bins.cat.categories:\n",
    "            Z[f\"bin_{lab}\"] = (bins == lab).astype(int)\n",
    "\n",
    "    return Z.astype(int)\n",
    "\n",
    "def iterative_split(X_ids: np.ndarray, Z: np.ndarray, test_size: float, seed: int):\n",
    "    \"\"\"\n",
    "    Multilabel iterative stratification Train/Test split on unique image_ids.\n",
    "    \"\"\"\n",
    "    if HAS_ITERSTRAT:\n",
    "        msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
    "        (train_idx, test_idx) = next(msss.split(X_ids, Z))\n",
    "        return X_ids[train_idx], X_ids[test_idx]\n",
    "    else:\n",
    "        # Fallback: warn + do a simple stratified shuffle on a scalar surrogate (label count).\n",
    "        print(\"[WARN] iterative-stratification not available; using surrogate stratification on label counts. \"\n",
    "              \"Install `iterative-stratification` for better matching.\", file=sys.stderr)\n",
    "        rng = np.random.default_rng(seed)\n",
    "        order = rng.permutation(len(X_ids))\n",
    "        cut = int(round(len(X_ids) * (1.0 - test_size)))\n",
    "        tr = X_ids[order[:cut]]\n",
    "        te = X_ids[order[cut:]]\n",
    "        return tr, te\n",
    "\n",
    "def make_splits(ids: pd.Index, Z: pd.DataFrame, seed: int = 42):\n",
    "    \"\"\"\n",
    "    60/10/30 on unique image_ids:\n",
    "      1) First take 30% Test\n",
    "      2) From remaining 70%, take 10/70 ≈ 14.2857% as Val to achieve 10% overall\n",
    "    \"\"\"\n",
    "    X = ids.to_numpy()\n",
    "    Zm = Z.loc[ids].to_numpy(dtype=int)\n",
    "\n",
    "    # Step 1: Test\n",
    "    train_ids, test_ids = iterative_split(X, Zm, test_size=0.30, seed=seed)\n",
    "\n",
    "    # Step 2: Val from remaining\n",
    "    remain_mask = pd.Index(X).isin(test_ids) == False\n",
    "    X_remain = X[remain_mask]\n",
    "    Z_remain = Z.loc[X_remain].to_numpy(dtype=int)\n",
    "    val_frac_of_remaining = 0.10 / 0.70  # == 1/7 ≈ 0.142857...\n",
    "    train_ids2, val_ids = iterative_split(X_remain, Z_remain, test_size=val_frac_of_remaining, seed=seed+1)\n",
    "\n",
    "    # Sanity\n",
    "    assert len(set(train_ids2) & set(val_ids)) == 0\n",
    "    assert len(set(test_ids) & set(val_ids)) == 0\n",
    "    assert len(set(test_ids) & set(train_ids2)) == 0\n",
    "\n",
    "    split = pd.Series(index=ids, data=\"train\")\n",
    "    split.loc[val_ids] = \"val\"\n",
    "    split.loc[test_ids] = \"test\"\n",
    "    return split\n",
    "\n",
    "def audit_report(split_s: pd.Series, targets):\n",
    "    y = targets[\"y_major\"]\n",
    "    nof = targets[\"no_finding_major\"]\n",
    "    nb = targets[\"n_boxes\"]\n",
    "    out = []\n",
    "\n",
    "    for s in [\"train\",\"val\",\"test\"]:\n",
    "        mask = (split_s == s)\n",
    "        n = int(mask.sum())\n",
    "        out.append(f\"[{s}] N={n}\")\n",
    "        # per-class positives\n",
    "        pos = y.loc[mask].sum()\n",
    "        prev = (pos / n).round(4)\n",
    "        lines = \", \".join([f\"c{cid}:{int(pos[cid])} ({prev[cid]:.3f})\" for cid in y.columns])\n",
    "        out.append(\"  per-class positives: \" + lines)\n",
    "        nf_prev = float(nof.loc[mask].mean())\n",
    "        out.append(f\"  no_finding prev: {nf_prev:.3f}\")\n",
    "        # annotation complexity\n",
    "        hist = nb.loc[mask].value_counts(bins=[-1,3,9,19,1000]).sort_index()\n",
    "        out.append(\"  n_boxes bins: \" + \", \".join([f\"{str(i)}:{int(v)}\" for i,v in hist.items()]))\n",
    "\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "def main(csv_path: str, out_dir: str = \"./splits_artifacts\", seed: int = 42):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    df = load_annotations(csv_path)\n",
    "\n",
    "    # Hard data integrity checks tied to dataset design\n",
    "    # 1) exactly 3 raters per image overall (counting No finding rows too)?\n",
    "    raters_per_image = df.groupby(\"image_id\")[\"rad_id\"].nunique()\n",
    "    if not raters_per_image.between(3,3).all():\n",
    "        bad = raters_per_image[raters_per_image != 3]\n",
    "        raise AssertionError(f\"Images without exactly 3 raters detected (n={len(bad)}). Example:\\n{bad.head()}\")\n",
    "\n",
    "    # 2) 'No finding' rows must have NaN boxes; positives must have numeric boxes\n",
    "    nf_bad = df[(df[\"class_id\"] == NO_FINDING_ID) & (~df[[\"x_min\",\"y_min\",\"x_max\",\"y_max\"]].isna().all(axis=1))]\n",
    "    pos_bad = df[(df[\"class_id\"] != NO_FINDING_ID) & (df[[\"x_min\",\"y_min\",\"x_max\",\"y_max\"]].isna().any(axis=1))]\n",
    "    if len(nf_bad):\n",
    "        raise AssertionError(f\"'No finding' rows with non-NaN boxes found: {len(nf_bad)}\")\n",
    "    if len(pos_bad):\n",
    "        raise AssertionError(f\"Positive rows with NaN bbox fields found: {len(pos_bad)}\")\n",
    "\n",
    "    # Build targets\n",
    "    targets = build_multilabel_targets(df)\n",
    "    ids = targets[\"y_major\"].index\n",
    "    Z = make_strat_matrix(targets, rare_bins=True)\n",
    "\n",
    "    # Splits\n",
    "    split_s = make_splits(ids, Z, seed=seed)\n",
    "\n",
    "    # Artifacts\n",
    "    patients = pd.DataFrame({\n",
    "        \"image_id\": ids,\n",
    "        \"hospital\": pd.Series(index=ids, dtype=\"object\")  # fill if you have site metadata\n",
    "    }).reset_index(drop=True)\n",
    "    patients.to_csv(os.path.join(out_dir, \"patients.csv\"), index=False)\n",
    "\n",
    "    splits = split_s.reset_index().rename(columns={0:\"split\"})\n",
    "    splits.columns = [\"image_id\",\"split\"]\n",
    "    splits.to_csv(os.path.join(out_dir, \"splits.csv\"), index=False)\n",
    "\n",
    "    # Audits\n",
    "    rep = audit_report(split_s, targets)\n",
    "    with open(os.path.join(out_dir, \"join_report.md\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"# Split and Join Audit\\n\\n\")\n",
    "        f.write(rep + \"\\n\")\n",
    "\n",
    "    print(rep)\n",
    "    print(f\"\\nWrote patients.csv and splits.csv to {out_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "089c555c-d8b8-4b06-9bf0-f27da53d67fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] N=9000\n",
      "  per-class positives: c0:1407 (0.156), c1:37 (0.004), c2:106 (0.012), c3:1090 (0.121), c4:73 (0.008), c5:91 (0.010), c6:147 (0.016), c7:328 (0.036), c8:243 (0.027), c9:217 (0.024), c10:381 (0.042), c11:529 (0.059), c12:35 (0.004), c13:610 (0.068)\n",
      "  no_finding prev: 0.707\n",
      "  n_boxes bins: (-1.001, 3.0]:6585, (3.0, 9.0]:1693, (9.0, 19.0]:631, (19.0, 1000.0]:91\n",
      "[val] N=1500\n",
      "  per-class positives: c0:235 (0.157), c1:6 (0.004), c2:18 (0.012), c3:182 (0.121), c4:12 (0.008), c5:15 (0.010), c6:25 (0.017), c7:55 (0.037), c8:41 (0.027), c9:36 (0.024), c10:63 (0.042), c11:88 (0.059), c12:6 (0.004), c13:102 (0.068)\n",
      "  no_finding prev: 0.707\n",
      "  n_boxes bins: (-1.001, 3.0]:1098, (3.0, 9.0]:282, (9.0, 19.0]:105, (19.0, 1000.0]:15\n",
      "[test] N=4500\n",
      "  per-class positives: c0:704 (0.156), c1:19 (0.004), c2:53 (0.012), c3:545 (0.121), c4:36 (0.008), c5:46 (0.010), c6:73 (0.016), c7:164 (0.036), c8:121 (0.027), c9:109 (0.024), c10:190 (0.042), c11:265 (0.059), c12:17 (0.004), c13:305 (0.068)\n",
      "  no_finding prev: 0.707\n",
      "  n_boxes bins: (-1.001, 3.0]:3293, (3.0, 9.0]:847, (9.0, 19.0]:315, (19.0, 1000.0]:45\n",
      "\n",
      "Wrote patients.csv and splits.csv to ./splits_artifacts\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"/workspace/df_224.csv\"\n",
    "out_dir = \"./splits_artifacts\"\n",
    "seed = 42\n",
    "main(csv_path, out_dir, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f860b5-92ab-4c7b-9875-ecf93497b4af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
