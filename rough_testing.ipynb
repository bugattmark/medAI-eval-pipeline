{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed98be43-aa4d-43ee-ae3d-2edfb2a8401f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kagglehub\n",
      "  Downloading kagglehub-0.3.12-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
      "Collecting tqdm (from kagglehub)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.1.31)\n",
      "Downloading kagglehub-0.3.12-py3-none-any.whl (67 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, kagglehub\n",
      "Successfully installed kagglehub-0.3.12 tqdm-4.67.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting iterative-stratification\n",
      "  Downloading iterative_stratification-0.1.9-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from iterative-stratification) (2.1.2)\n",
      "Collecting scipy (from iterative-stratification)\n",
      "  Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting scikit-learn (from iterative-stratification)\n",
      "  Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->iterative-stratification)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->iterative-stratification)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading iterative_stratification-0.1.9-py3-none-any.whl (8.5 kB)\n",
      "Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.4/35.4 MB\u001b[0m \u001b[31m141.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, iterative-stratification\n",
      "Successfully installed iterative-stratification-0.1.9 joblib-1.5.1 scikit-learn-1.7.1 scipy-1.16.1 threadpoolctl-3.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub\n",
    "!pip install iterative-stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe24c54a-de60-4cb8-9e5f-21f2941ed19e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kagglehub'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkagglehub\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Download latest version\u001b[39;00m\n\u001b[32m      4\u001b[39m path = kagglehub.dataset_download(\u001b[33m\"\u001b[39m\u001b[33mxhlulu/vinbigdata-chest-xray-resized-png-256x256\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'kagglehub'"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"xhlulu/vinbigdata-chest-xray-resized-png-256x256\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7babdcf8-a0c0-4e5c-ac24-4048820b0212",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv /root/.cache/kagglehub/datasets/xhlulu/vinbigdata-chest-xray-resized-png-256x256/versions/1 /workspace/256data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f8b23fe-9775-4e09-8d03-a1ad0317b335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.3.1 pytz-2025.2 tzdata-2025.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93e64204-c71f-4220-a8b9-87d8aa2f5b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple, Optional\n",
    "\n",
    "NO_FINDING = 14\n",
    "\n",
    "def warp_box(box, sx, sy): #scale factor x, scale factor y\n",
    "    if box is None: return None\n",
    "    x1,y1,x2,y2 = box\n",
    "    return [x1*sx, y1*sy, x2*sx, y2*sy]\n",
    "\n",
    "def clamp_box(box, w, h):\n",
    "    if box is None: return None\n",
    "    x1,y1,x2,y2 = box\n",
    "    x1 = max(0.0, min(w, x1)); y1 = max(0.0, min(h, y1))\n",
    "    x2 = max(0.0, min(w, x2)); y2 = max(0.0, min(h, y2))\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return None\n",
    "    return [x1,y1,x2,y2]\n",
    "\n",
    "def transform_boxes_to_224(\n",
    "    df: pd.DataFrame,\n",
    "    mode: str = \"from_256\",  # or \"from_original\"\n",
    "    per_image_src_size: Optional[Dict[str, Tuple[int,int]]] = None,\n",
    "    class_no_finding: int = NO_FINDING,\n",
    "    drop_degenerate: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Expects columns: image_id, class_id, x_min, y_min, x_max, y_max\n",
    "    Returns a copy with boxes mapped to a 224x224 image via warp.\n",
    "    \"\"\"\n",
    "    req_cols = {\"image_id\",\"class_id\",\"x_min\",\"y_min\",\"x_max\",\"y_max\"}\n",
    "    missing = req_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "    out = df.copy()\n",
    "    w_target = h_target = 224\n",
    "    scale_256_to_224 = w_target / 256.0  # == 224/256\n",
    "\n",
    "    new_x1,new_y1,new_x2,new_y2,keep = [],[],[],[],[]\n",
    "\n",
    "    for row in out.itertuples(index=False):\n",
    "        img = getattr(row, \"image_id\")\n",
    "        cls = getattr(row, \"class_id\")\n",
    "        x1 = getattr(row, \"x_min\"); y1 = getattr(row, \"y_min\")\n",
    "        x2 = getattr(row, \"x_max\"); y2 = getattr(row, \"y_max\")\n",
    "\n",
    "        # Keep NaNs for No finding\n",
    "        if cls == class_no_finding or any(pd.isna(v) for v in (x1,y1,x2,y2)):\n",
    "            new_x1.append(np.nan); new_y1.append(np.nan)\n",
    "            new_x2.append(np.nan); new_y2.append(np.nan)\n",
    "            keep.append(True)\n",
    "            continue\n",
    "\n",
    "        box = [float(x1), float(y1), float(x2), float(y2)]\n",
    "\n",
    "        if mode == \"from_original\":\n",
    "            if per_image_src_size is None or img not in per_image_src_size:\n",
    "                raise ValueError(f\"Missing original size for image_id={img}\")\n",
    "            W0,H0 = per_image_src_size[img]\n",
    "            # warp original -> 256\n",
    "            sx1 = 256.0 / float(W0); sy1 = 256.0 / float(H0)\n",
    "            box = warp_box(box, sx1, sy1)\n",
    "            box = clamp_box(box, 256.0, 256.0)\n",
    "            if box is None:\n",
    "                if drop_degenerate: keep.append(False); continue\n",
    "                else:\n",
    "                    new_x1.append(np.nan); new_y1.append(np.nan)\n",
    "                    new_x2.append(np.nan); new_y2.append(np.nan)\n",
    "                    keep.append(True); continue\n",
    "\n",
    "        elif mode != \"from_256\":\n",
    "            raise ValueError(\"mode must be 'from_256' or 'from_original'\")\n",
    "\n",
    "        # warp 256 -> 224\n",
    "        box = warp_box(box, scale_256_to_224, scale_256_to_224)\n",
    "        box = clamp_box(box, w_target, h_target)\n",
    "        if box is None:\n",
    "            if drop_degenerate: keep.append(False); continue\n",
    "            else:\n",
    "                new_x1.append(np.nan); new_y1.append(np.nan)\n",
    "                new_x2.append(np.nan); new_y2.append(np.nan)\n",
    "                keep.append(True); continue\n",
    "\n",
    "        x1,y1,x2,y2 = box\n",
    "        new_x1.append(x1); new_y1.append(y1)\n",
    "        new_x2.append(x2); new_y2.append(y2)\n",
    "        keep.append(True)\n",
    "\n",
    "    out = out.loc[keep].copy()\n",
    "    out[\"x_min\"] = new_x1[:len(out)]\n",
    "    out[\"y_min\"] = new_y1[:len(out)]\n",
    "    out[\"x_max\"] = new_x2[:len(out)]\n",
    "    out[\"y_max\"] = new_y2[:len(out)]\n",
    "    return out\n",
    "\n",
    "# --- Examples ---\n",
    "\n",
    "# 1) Boxes already on 256 grid\n",
    "# df_224 = transform_boxes_to_224(df, mode=\"from_256\")\n",
    "\n",
    "# 2) Boxes in original DICOM coords; you must supply per-image (W0,H0):\n",
    "# per_image_src_size = dict(zip(meta_df.image_id, zip(meta_df.dim1, meta_df.dim0)))  # note: (W0,H0)\n",
    "# df_224 = transform_boxes_to_224(df, mode=\"from_original\", per_image_src_size=per_image_src_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fb34b4e-6d36-467a-a44d-d31baf37c592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 322 ms, sys: 43.3 ms, total: 365 ms\n",
      "Wall time: 367 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "meta_df = pd.read_csv('train.csv')\n",
    "per_image_src_size = dict(zip(meta_df.image_id, zip(meta_df.width, meta_df.height)))  # note: (W0,H0)\n",
    "df_224 = transform_boxes_to_224(meta_df, mode=\"from_original\", per_image_src_size=per_image_src_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf067ade-4956-48e9-8dfd-037ed6a7f342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               image_id          class_name  class_id rad_id  \\\n",
      "0      50a418190bc3fb1ef1633bf9678929b3          No finding        14    R11   \n",
      "1      21a10246a5ec7af151081d0cd6d65dc9          No finding        14     R7   \n",
      "2      9a5094b2563a1ef3ff50dc5c7ff71345        Cardiomegaly         3    R10   \n",
      "3      051132a778e61a86eb147c7c6f564dfe  Aortic enlargement         0    R10   \n",
      "4      063319de25ce7edb9b1c6b8881290140          No finding        14    R10   \n",
      "...                                 ...                 ...       ...    ...   \n",
      "67909  936fd5cff1c058d39817a08f58b72cae          No finding        14     R1   \n",
      "67910  ca7e72954550eeb610fe22bf0244b7fa          No finding        14     R1   \n",
      "67911  aa17d5312a0fb4a2939436abca7f9579          No finding        14     R8   \n",
      "67912  4b56bc6d22b192f075f13231419dfcc8        Cardiomegaly         3     R8   \n",
      "67913  5e272e3adbdaafb07a7e84a9e62b1a4c          No finding        14    R16   \n",
      "\n",
      "            x_min       y_min       x_max       y_max  width  height  \n",
      "0             NaN         NaN         NaN         NaN   2332    2580  \n",
      "1             NaN         NaN         NaN         NaN   2954    3159  \n",
      "2       74.415385  131.849315  178.015385  175.575342   2080    2336  \n",
      "3      122.888889   57.788889  156.625000   79.255556   2304    2880  \n",
      "4             NaN         NaN         NaN         NaN   2540    3072  \n",
      "...           ...         ...         ...         ...    ...     ...  \n",
      "67909         NaN         NaN         NaN         NaN   2444    3200  \n",
      "67910         NaN         NaN         NaN         NaN   1994    2430  \n",
      "67911         NaN         NaN         NaN         NaN   2048    2500  \n",
      "67912   87.756098  107.498039  191.219512  143.952941   1968    2040  \n",
      "67913         NaN         NaN         NaN         NaN   2048    2500  \n",
      "\n",
      "[67914 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_224)\n",
    "df_224.to_csv(\"df_224.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "940dcd06-b2c9-4caa-8e19-4833690453d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_vindr_like.py\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import unicodedata\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Try to use iterative stratification; fall back to a simple stratifier with a warning.\n",
    "try:\n",
    "    from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "    HAS_ITERSTRAT = True\n",
    "except Exception:\n",
    "    HAS_ITERSTRAT = False\n",
    "\n",
    "NO_FINDING_ID = 14  # dataset convention\n",
    "\n",
    "def nfc_norm(s: str) -> str:\n",
    "    # NFC normalize + lowercase + strip spaces; \"canonical\" filename normalization\n",
    "    return unicodedata.normalize(\"NFC\", s).strip().lower()\n",
    "\n",
    "def load_annotations(csv_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # minimal schema checks\n",
    "    need = {\"image_id\",\"class_id\",\"rad_id\",\"x_min\",\"y_min\",\"x_max\",\"y_max\"}\n",
    "    miss = need - set(df.columns)\n",
    "    if miss:\n",
    "        raise ValueError(f\"CSV missing columns: {miss}\")\n",
    "    # normalize IDs\n",
    "    df[\"image_id\"] = df[\"image_id\"].astype(str).map(nfc_norm)\n",
    "    df[\"rad_id\"] = df[\"rad_id\"].astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "def build_multilabel_targets(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Build per-image targets.\n",
    "\n",
    "    Majority rule per class c in [0..13]: positive if >=2 distinct raters annotated at least one bbox of class c.\n",
    "    Soft label per class: (#distinct-raters-positive)/3 in {0, 1/3, 2/3, 1}.\n",
    "\n",
    "    no_finding_majority: 1 if >=2 raters said No finding AND no other class has majority positive.\n",
    "    \"\"\"\n",
    "    # Deduplicate boxes per (image, class, rater) so a rater with multiple boxes for same class counts once\n",
    "    df_pos = df[df[\"class_id\"] != NO_FINDING_ID].copy()\n",
    "    df_pos = df_pos.dropna(subset=[\"x_min\",\"y_min\",\"x_max\",\"y_max\"])\n",
    "    df_pos[\"rad_int\"] = df_pos[\"rad_id\"].str[1:].astype(int) if df_pos[\"rad_id\"].str.startswith(\"R\").all() else df_pos[\"rad_id\"]\n",
    "\n",
    "    # Rater -> class presence\n",
    "    raters_per_class = (\n",
    "        df_pos.groupby([\"image_id\",\"class_id\"])[\"rad_id\"]\n",
    "        .nunique()\n",
    "        .rename(\"rater_count\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Initialize label tables\n",
    "    images = sorted(df[\"image_id\"].unique())\n",
    "    classes = list(range(0, NO_FINDING_ID))  # 0..13\n",
    "    idx = pd.Index(images, name=\"image_id\")\n",
    "    y_major = pd.DataFrame(0, index=idx, columns=classes, dtype=np.int8)\n",
    "    y_soft   = pd.DataFrame(0.0, index=idx, columns=classes, dtype=float)\n",
    "\n",
    "    # Fill per-class labels\n",
    "    for _, row in raters_per_class.iterrows():\n",
    "        img, cid, rc = row[\"image_id\"], int(row[\"class_id\"]), int(row[\"rater_count\"])\n",
    "        if cid in classes:\n",
    "            y_soft.at[img, cid] = min(1.0, rc / 3.0)  # in {1/3, 2/3, 1} (or 0 if absent)\n",
    "            y_major.at[img, cid] = 1 if rc >= 2 else max(y_major.at[img, cid], 0)\n",
    "\n",
    "    # Ensure zeros where absent\n",
    "    y_soft = y_soft.fillna(0.0)\n",
    "    y_major = y_major.fillna(0).astype(np.int8)\n",
    "\n",
    "    # No finding counts per image (how many raters said \"No finding\")\n",
    "    nf_counts = (\n",
    "        df[df[\"class_id\"] == NO_FINDING_ID]\n",
    "        .groupby(\"image_id\")[\"rad_id\"].nunique()\n",
    "        .reindex(idx, fill_value=0)\n",
    "    )\n",
    "    # no_finding majority is 1 only if ≥2 raters say No finding and NO other class has majority positive\n",
    "    no_find_major = ((nf_counts >= 2) & (y_major.sum(axis=1) == 0)).astype(np.int8)\n",
    "\n",
    "    # Auxiliary: number of (unique) boxes per image (any class, any rater)\n",
    "    n_boxes = (\n",
    "        df_pos.groupby(\"image_id\")[[\"x_min\",\"y_min\",\"x_max\",\"y_max\",\"rad_id\",\"class_id\"]]\n",
    "        .apply(lambda g: len(g))\n",
    "        .reindex(idx, fill_value=0)\n",
    "        .rename(\"n_boxes\")\n",
    "    )\n",
    "\n",
    "    targets = {\n",
    "        \"y_major\": y_major,         # (N, 14) ints 0/1\n",
    "        \"y_soft\": y_soft,           # (N, 14) floats in [0,1]\n",
    "        \"no_finding_major\": no_find_major,  # (N,) ints 0/1\n",
    "        \"n_boxes\": n_boxes,         # (N,) ints\n",
    "    }\n",
    "    return targets\n",
    "\n",
    "def make_strat_matrix(targets, rare_bins=True):\n",
    "    \"\"\"\n",
    "    Build a multilabel stratification matrix Z (N, K).\n",
    "\n",
    "    Columns include:\n",
    "      - 14 binary majority labels (classes 0,1,2,..,13)\n",
    "      - one 'no_finding' column\n",
    "      - optional buckets of n_boxes to keep annotation complexity roughly matched\n",
    "    \"\"\"\n",
    "    Z = targets[\"y_major\"].copy()\n",
    "    Z[\"no_finding\"] = targets[\"no_finding_major\"]\n",
    "\n",
    "    if rare_bins:\n",
    "        # Bucketize n_boxes (annotation complexity) as a weak feature\n",
    "        nb = targets[\"n_boxes\"].clip(upper=60)\n",
    "        # simple bins: 0-3, 4-9, 10-19, >=20\n",
    "        bins = pd.cut(nb, bins=[-1,3,9,19,1000], labels=[\"b0_3\",\"b4_9\",\"b10_19\",\"b20p\"])\n",
    "        for lab in bins.cat.categories:\n",
    "            Z[f\"bin_{lab}\"] = (bins == lab).astype(int)\n",
    "\n",
    "    return Z.astype(int)\n",
    "\n",
    "def iterative_split(X_ids: np.ndarray, Z: np.ndarray, test_size: float, seed: int):\n",
    "    \"\"\"\n",
    "    Multilabel iterative stratification Train/Test split on unique image_ids.\n",
    "    \"\"\"\n",
    "    if HAS_ITERSTRAT:\n",
    "        msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
    "        (train_idx, test_idx) = next(msss.split(X_ids, Z))\n",
    "        return X_ids[train_idx], X_ids[test_idx]\n",
    "    else:\n",
    "        # Fallback: warn + do a simple stratified shuffle on a scalar surrogate (label count).\n",
    "        print(\"[WARN] iterative-stratification not available; using surrogate stratification on label counts. \"\n",
    "              \"Install `iterative-stratification` for better matching.\", file=sys.stderr)\n",
    "        rng = np.random.default_rng(seed)\n",
    "        order = rng.permutation(len(X_ids))\n",
    "        cut = int(round(len(X_ids) * (1.0 - test_size)))\n",
    "        tr = X_ids[order[:cut]]\n",
    "        te = X_ids[order[cut:]]\n",
    "        return tr, te\n",
    "\n",
    "def make_splits(ids: pd.Index, Z: pd.DataFrame, seed: int = 42):\n",
    "    \"\"\"\n",
    "    60/10/30 on unique image_ids:\n",
    "      1) First take 30% Test\n",
    "      2) From remaining 70%, take 10/70 ≈ 14.2857% as Val to achieve 10% overall\n",
    "    \"\"\"\n",
    "    X = ids.to_numpy()\n",
    "    Zm = Z.loc[ids].to_numpy(dtype=int)\n",
    "\n",
    "    # Step 1: Test\n",
    "    train_ids, test_ids = iterative_split(X, Zm, test_size=0.30, seed=seed)\n",
    "\n",
    "    # Step 2: Val from remaining\n",
    "    remain_mask = pd.Index(X).isin(test_ids) == False\n",
    "    X_remain = X[remain_mask]\n",
    "    Z_remain = Z.loc[X_remain].to_numpy(dtype=int)\n",
    "    val_frac_of_remaining = 0.10 / 0.70  # == 1/7 ≈ 0.142857...\n",
    "    train_ids2, val_ids = iterative_split(X_remain, Z_remain, test_size=val_frac_of_remaining, seed=seed+1)\n",
    "\n",
    "    # Sanity\n",
    "    assert len(set(train_ids2) & set(val_ids)) == 0\n",
    "    assert len(set(test_ids) & set(val_ids)) == 0\n",
    "    assert len(set(test_ids) & set(train_ids2)) == 0\n",
    "\n",
    "    split = pd.Series(index=ids, data=\"train\")\n",
    "    split.loc[val_ids] = \"val\"\n",
    "    split.loc[test_ids] = \"test\"\n",
    "    return split\n",
    "\n",
    "def audit_report(split_s: pd.Series, targets):\n",
    "    y = targets[\"y_major\"]\n",
    "    nof = targets[\"no_finding_major\"]\n",
    "    nb = targets[\"n_boxes\"]\n",
    "    out = []\n",
    "\n",
    "    for s in [\"train\",\"val\",\"test\"]:\n",
    "        mask = (split_s == s)\n",
    "        n = int(mask.sum())\n",
    "        out.append(f\"[{s}] N={n}\")\n",
    "        # per-class positives\n",
    "        pos = y.loc[mask].sum()\n",
    "        prev = (pos / n).round(4)\n",
    "        lines = \", \".join([f\"c{cid}:{int(pos[cid])} ({prev[cid]:.3f})\" for cid in y.columns])\n",
    "        out.append(\"  per-class positives: \" + lines)\n",
    "        nf_prev = float(nof.loc[mask].mean())\n",
    "        out.append(f\"  no_finding prev: {nf_prev:.3f}\")\n",
    "        # annotation complexity\n",
    "        hist = nb.loc[mask].value_counts(bins=[-1,3,9,19,1000]).sort_index()\n",
    "        out.append(\"  n_boxes bins: \" + \", \".join([f\"{str(i)}:{int(v)}\" for i,v in hist.items()]))\n",
    "\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "def main(csv_path: str, out_dir: str = \"./splits_artifacts\", seed: int = 42):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    df = load_annotations(csv_path)\n",
    "\n",
    "    # Hard data integrity checks tied to dataset design\n",
    "    # 1) exactly 3 raters per image overall (counting No finding rows too)?\n",
    "    raters_per_image = df.groupby(\"image_id\")[\"rad_id\"].nunique()\n",
    "    if not raters_per_image.between(3,3).all():\n",
    "        bad = raters_per_image[raters_per_image != 3]\n",
    "        raise AssertionError(f\"Images without exactly 3 raters detected (n={len(bad)}). Example:\\n{bad.head()}\")\n",
    "\n",
    "    # 2) 'No finding' rows must have NaN boxes; positives must have numeric boxes\n",
    "    nf_bad = df[(df[\"class_id\"] == NO_FINDING_ID) & (~df[[\"x_min\",\"y_min\",\"x_max\",\"y_max\"]].isna().all(axis=1))]\n",
    "    pos_bad = df[(df[\"class_id\"] != NO_FINDING_ID) & (df[[\"x_min\",\"y_min\",\"x_max\",\"y_max\"]].isna().any(axis=1))]\n",
    "    if len(nf_bad):\n",
    "        raise AssertionError(f\"'No finding' rows with non-NaN boxes found: {len(nf_bad)}\")\n",
    "    if len(pos_bad):\n",
    "        raise AssertionError(f\"Positive rows with NaN bbox fields found: {len(pos_bad)}\")\n",
    "\n",
    "    # Build targets\n",
    "    targets = build_multilabel_targets(df)\n",
    "    ids = targets[\"y_major\"].index\n",
    "    Z = make_strat_matrix(targets, rare_bins=True)\n",
    "\n",
    "    # Splits\n",
    "    split_s = make_splits(ids, Z, seed=seed)\n",
    "\n",
    "    # Artifacts\n",
    "    patients = pd.DataFrame({\n",
    "        \"image_id\": ids,\n",
    "        \"hospital\": pd.Series(index=ids, dtype=\"object\")  # fill if you have site metadata\n",
    "    }).reset_index(drop=True)\n",
    "    patients.to_csv(os.path.join(out_dir, \"patients.csv\"), index=False)\n",
    "\n",
    "    splits = split_s.reset_index().rename(columns={0:\"split\"})\n",
    "    splits.columns = [\"image_id\",\"split\"]\n",
    "    splits.to_csv(os.path.join(out_dir, \"splits.csv\"), index=False)\n",
    "\n",
    "    # Audits\n",
    "    rep = audit_report(split_s, targets)\n",
    "    with open(os.path.join(out_dir, \"join_report.md\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"# Split and Join Audit\\n\\n\")\n",
    "        f.write(rep + \"\\n\")\n",
    "\n",
    "    print(rep)\n",
    "    print(f\"\\nWrote patients.csv and splits.csv to {out_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "089c555c-d8b8-4b06-9bf0-f27da53d67fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] N=9000\n",
      "  per-class positives: c0:1407 (0.156), c1:37 (0.004), c2:106 (0.012), c3:1090 (0.121), c4:73 (0.008), c5:91 (0.010), c6:147 (0.016), c7:328 (0.036), c8:243 (0.027), c9:217 (0.024), c10:381 (0.042), c11:529 (0.059), c12:35 (0.004), c13:610 (0.068)\n",
      "  no_finding prev: 0.707\n",
      "  n_boxes bins: (-1.001, 3.0]:6585, (3.0, 9.0]:1693, (9.0, 19.0]:631, (19.0, 1000.0]:91\n",
      "[val] N=1500\n",
      "  per-class positives: c0:235 (0.157), c1:6 (0.004), c2:18 (0.012), c3:182 (0.121), c4:12 (0.008), c5:15 (0.010), c6:25 (0.017), c7:55 (0.037), c8:41 (0.027), c9:36 (0.024), c10:63 (0.042), c11:88 (0.059), c12:6 (0.004), c13:102 (0.068)\n",
      "  no_finding prev: 0.707\n",
      "  n_boxes bins: (-1.001, 3.0]:1098, (3.0, 9.0]:282, (9.0, 19.0]:105, (19.0, 1000.0]:15\n",
      "[test] N=4500\n",
      "  per-class positives: c0:704 (0.156), c1:19 (0.004), c2:53 (0.012), c3:545 (0.121), c4:36 (0.008), c5:46 (0.010), c6:73 (0.016), c7:164 (0.036), c8:121 (0.027), c9:109 (0.024), c10:190 (0.042), c11:265 (0.059), c12:17 (0.004), c13:305 (0.068)\n",
      "  no_finding prev: 0.707\n",
      "  n_boxes bins: (-1.001, 3.0]:3293, (3.0, 9.0]:847, (9.0, 19.0]:315, (19.0, 1000.0]:45\n",
      "\n",
      "Wrote patients.csv and splits.csv to ./splits_artifacts\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"/workspace/medAI-eval-pipeline/df_224.csv\"\n",
    "out_dir = \"./splits_artifacts\"\n",
    "seed = 42\n",
    "main(csv_path, out_dir, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88f860b5-92ab-4c7b-9875-ecf93497b4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_datasets.py\n",
    "import os\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "NO_FINDING_ID = 14\n",
    "DISEASE_CLASSES = list(range(0, NO_FINDING_ID))  # 0..13\n",
    "\n",
    "# ---------- paths ----------\n",
    "DF224_PATH   = \"./df_224.csv\"        # post-geometry boxes on 224 grid\n",
    "SPLITS_PATH  = \"./splits_artifacts/splits.csv\"  # image_id -> split\n",
    "OUT_DIR      = \"./dataset_artifacts\"  # where to write classification.csv, bbox.csv, joined_view.csv\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def nfc_norm(s: str) -> str:\n",
    "    return unicodedata.normalize(\"NFC\", str(s)).strip().lower()\n",
    "\n",
    "def require(df: pd.DataFrame, cols):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "def load_df224(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    require(df, [\"image_id\",\"class_id\",\"rad_id\",\"x_min\",\"y_min\",\"x_max\",\"y_max\"])\n",
    "    # normalize identifiers\n",
    "    df[\"image_id\"] = df[\"image_id\"].astype(str).map(nfc_norm)\n",
    "    df[\"rad_id\"]   = df[\"rad_id\"].astype(str).str.strip().str.upper()\n",
    "    # optional source size columns may exist as 'width','height'\n",
    "    if \"width\" in df.columns and \"height\" in df.columns:\n",
    "        df = df.rename(columns={\"width\":\"width_src\",\"height\":\"height_src\"})\n",
    "    return df\n",
    "\n",
    "def load_splits(path: str) -> pd.DataFrame:\n",
    "    sp = pd.read_csv(path)\n",
    "    require(sp, [\"image_id\",\"split\"])\n",
    "    sp[\"image_id\"] = sp[\"image_id\"].astype(str).map(nfc_norm)\n",
    "    sp[\"split\"] = sp[\"split\"].astype(str)\n",
    "    # assert one split per id\n",
    "    dup = sp[\"image_id\"].duplicated(keep=False)\n",
    "    if dup.any():\n",
    "        raise AssertionError(f\"Duplicate image_id in splits.csv (n={dup.sum()}).\")\n",
    "    return sp\n",
    "\n",
    "# ---------- schema checks ----------\n",
    "def run_schema_checks(df: pd.DataFrame, splits: pd.DataFrame):\n",
    "    # 1) exactly 3 distinct raters per image\n",
    "    raters_per_image = df.groupby(\"image_id\")[\"rad_id\"].nunique()\n",
    "    bad = raters_per_image[raters_per_image != 3]\n",
    "    if len(bad):\n",
    "        raise AssertionError(f\"Images without exactly 3 raters: {len(bad)}. Example:\\n{bad.head()}\")\n",
    "\n",
    "    # 2) No finding rows must have all-NaN boxes; disease rows must have none-NaN boxes\n",
    "    nf_bad  = df[(df[\"class_id\"] == NO_FINDING_ID) & (~df[[\"x_min\",\"y_min\",\"x_max\",\"y_max\"]].isna().all(axis=1))]\n",
    "    pos_bad = df[(df[\"class_id\"] != NO_FINDING_ID) & (df[[\"x_min\",\"y_min\",\"x_max\",\"y_max\"]].isna().any(axis=1))]\n",
    "    if len(nf_bad):  raise AssertionError(f\"'No finding' rows with non-NaN boxes: {len(nf_bad)}\")\n",
    "    if len(pos_bad): raise AssertionError(f\"Disease rows with NaN box fields: {len(pos_bad)}\")\n",
    "\n",
    "    # 3) per-image annotation count within [3, 57]\n",
    "    ann_counts = df.groupby(\"image_id\").size()\n",
    "    out_of_range = ann_counts[(ann_counts < 3) | (ann_counts > 57)]\n",
    "    if len(out_of_range):\n",
    "        raise AssertionError(f\"Images with annotation count outside [3,57]: {len(out_of_range)}\")\n",
    "\n",
    "    # 4) coverage + exclusivity of splits\n",
    "    ids_df = set(df[\"image_id\"].unique())\n",
    "    ids_sp = set(splits[\"image_id\"].unique())\n",
    "    if ids_df != ids_sp:\n",
    "        only_df = len(ids_df - ids_sp); only_sp = len(ids_sp - ids_df)\n",
    "        raise AssertionError(f\"Split/image mismatch. only_in_df224={only_df}, only_in_splits={only_sp}\")\n",
    "\n",
    "# ---------- label building ----------\n",
    "def build_labels(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      labels_major (DataFrame: image_id index, columns maj_c0..maj_c13),\n",
    "      labels_soft   (DataFrame: image_id index, columns soft_c0..soft_c13),\n",
    "      no_finding_major (Series: 0/1),\n",
    "      n_boxes (Series: int), width_src/height_src if available (DataFrame)\n",
    "    \"\"\"\n",
    "    images = pd.Index(sorted(df[\"image_id\"].unique()), name=\"image_id\")\n",
    "\n",
    "    # positives with valid boxes\n",
    "    df_pos = df[(df[\"class_id\"] != NO_FINDING_ID) & (~df[[\"x_min\",\"y_min\",\"x_max\",\"y_max\"]].isna().any(axis=1))].copy()\n",
    "\n",
    "    # distinct raters per (image, class)\n",
    "    rater_counts = (\n",
    "        df_pos.groupby([\"image_id\",\"class_id\"])[\"rad_id\"]\n",
    "        .nunique()\n",
    "        .rename(\"rc\").reset_index()\n",
    "    )\n",
    "\n",
    "    # initialize tables\n",
    "    maj_cols  = [f\"maj_c{i}\"  for i in DISEASE_CLASSES]\n",
    "    soft_cols = [f\"soft_c{i}\" for i in DISEASE_CLASSES]\n",
    "    y_major = pd.DataFrame(0, index=images, columns=maj_cols, dtype=np.int8)\n",
    "    y_soft  = pd.DataFrame(0.0, index=images, columns=soft_cols, dtype=float)\n",
    "\n",
    "    # fill from rater counts\n",
    "    for _, row in rater_counts.iterrows():\n",
    "        cid = int(row[\"class_id\"])\n",
    "        if cid in DISEASE_CLASSES:\n",
    "            rc = int(row[\"rc\"])\n",
    "            y_soft.loc[row[\"image_id\"], f\"soft_c{cid}\"] = min(1.0, rc/3.0)\n",
    "            y_major.loc[row[\"image_id\"], f\"maj_c{cid}\"] = 1 if rc >= 2 else 0\n",
    "\n",
    "    # no_finding majority = (>=2 raters chose NF) AND (no disease has majority)\n",
    "    nf_counts = (\n",
    "        df[df[\"class_id\"] == NO_FINDING_ID]\n",
    "        .groupby(\"image_id\")[\"rad_id\"].nunique()\n",
    "        .reindex(images, fill_value=0)\n",
    "    )\n",
    "    any_disease_major = (y_major.sum(axis=1) > 0)\n",
    "    no_finding_major = ((nf_counts >= 2) & (~any_disease_major)).astype(np.int8)\n",
    "\n",
    "    # n_boxes = count of positive annotations (all classes, all raters) after 224 transform\n",
    "    n_boxes = df_pos.groupby(\"image_id\").size().reindex(images, fill_value=0).astype(int)\n",
    "\n",
    "    # optional source dims\n",
    "    dims = pd.DataFrame(index=images)\n",
    "    if \"width_src\" in df.columns and \"height_src\" in df.columns:\n",
    "        dims = df.groupby(\"image_id\")[[\"width_src\",\"height_src\"]].first().reindex(images)\n",
    "\n",
    "    return y_major, y_soft, no_finding_major, n_boxes, dims\n",
    "\n",
    "# ---------- build datasets ----------\n",
    "def main():\n",
    "    df = load_df224(DF224_PATH)\n",
    "    splits = load_splits(SPLITS_PATH)\n",
    "    run_schema_checks(df, splits)\n",
    "\n",
    "    # labels\n",
    "    y_major, y_soft, no_finding_major, n_boxes, dims = build_labels(df)\n",
    "\n",
    "    # classification.csv (one row per image_id)\n",
    "    cls = pd.DataFrame(index=y_major.index)\n",
    "    cls[\"split\"] = splits.set_index(\"image_id\")[\"split\"].reindex(cls.index)\n",
    "    cls = cls.join(y_major).join(y_soft)\n",
    "    cls[\"no_finding_major\"] = no_finding_major\n",
    "    cls[\"n_boxes\"] = n_boxes\n",
    "    cls[\"width_224\"] = 224\n",
    "    cls[\"height_224\"] = 224\n",
    "    if not dims.empty:\n",
    "        cls = cls.join(dims)\n",
    "    cls = cls.reset_index()\n",
    "    cls.to_csv(os.path.join(OUT_DIR, \"classification.csv\"), index=False)\n",
    "\n",
    "    # bbox.csv (one row per annotation on 224 grid)\n",
    "    # keep essential, plus optional src dims if present\n",
    "    keep_cols = [\"image_id\",\"class_id\",\"rad_id\",\"x_min\",\"y_min\",\"x_max\",\"y_max\"]\n",
    "    if \"class_name\" in df.columns: keep_cols.append(\"class_name\")\n",
    "    if \"width_src\" in df.columns:  keep_cols += [\"width_src\",\"height_src\"]\n",
    "    bbox = df.loc[:, keep_cols].copy()\n",
    "    bbox.to_csv(os.path.join(OUT_DIR, \"bbox.csv\"), index=False)\n",
    "\n",
    "    # joined_view.csv (for manual review only)\n",
    "    joined = bbox.merge(cls, on=\"image_id\", how=\"left\")\n",
    "    joined.to_csv(os.path.join(OUT_DIR, \"joined_view.csv\"), index=False)\n",
    "\n",
    "    # quick console summary\n",
    "    print(f\"Wrote:\\n  {os.path.join(OUT_DIR,'classification.csv')}\\n  {os.path.join(OUT_DIR,'bbox.csv')}\\n  {os.path.join(OUT_DIR,'joined_view.csv')}\")\n",
    "    # sanity: ensure every bbox row has a classification row\n",
    "    missing = joined[\"split\"].isna().sum()\n",
    "    if missing:\n",
    "        raise AssertionError(f\"{missing} bbox rows lack classification info (join failure).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "264f3699-aab1-49d0-8a2d-8eec9ac72b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote:\n",
      "  ./dataset_artifacts/classification.csv\n",
      "  ./dataset_artifacts/bbox.csv\n",
      "  ./dataset_artifacts/joined_view.csv\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "24435adc-7d85-4ea3-916e-6028cfbda6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"/workspace/medAI-eval-pipeline/dataset_artifacts/classification.csv\")\n",
    "df2 = pd.read_csv(\"/workspace/medAI-eval-pipeline/df_224.csv\")\n",
    "len(df1) == df2['image_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "be3f9c3f-2954-4128-949b-b6c38fb255e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['image_id'].duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f505cd18-e53a-4294-9894-d8cfcab066da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAYYYY\n"
     ]
    }
   ],
   "source": [
    "print('YAYYYY') if not df1['image_id'].duplicated().any() else print('NOOOOO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c62cc02-42b8-4008-a029-5f17dc3286e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['image_id', 'split', 'maj_c0', 'maj_c1', 'maj_c2', 'maj_c3', 'maj_c4',\n",
       "       'maj_c5', 'maj_c6', 'maj_c7', 'maj_c8', 'maj_c9', 'maj_c10', 'maj_c11',\n",
       "       'maj_c12', 'maj_c13', 'soft_c0', 'soft_c1', 'soft_c2', 'soft_c3',\n",
       "       'soft_c4', 'soft_c5', 'soft_c6', 'soft_c7', 'soft_c8', 'soft_c9',\n",
       "       'soft_c10', 'soft_c11', 'soft_c12', 'soft_c13', 'no_finding_major',\n",
       "       'n_boxes', 'width_224', 'height_224', 'width_src', 'height_src'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08f75a5e-b9a1-4b17-b9eb-d93dc185997d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['image_id', 'class_name', 'class_id', 'rad_id', 'x_min', 'y_min',\n",
       "       'x_max', 'y_max', 'width', 'height'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9a53b1e0-9fea-4e68-a354-c50f30820237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check classification csv\n",
    "\n",
    "# to_check = [f\"maj_c{x}\" for x in range(14)]\n",
    "# mask = df1[to_check].sum(axis=1) > 1\n",
    "# # print(df1.loc[mask, 'no_finding_major'])\n",
    "# # print(len(df1.loc[mask, 'no_finding_major']))\n",
    "# # assert (df1.loc[mask, 'no_finding_major'] == 0).all(), \"error\" #no output so sum>0 => nf==0\n",
    "\n",
    "# mask2 = df1[to_check].sum(axis=1) == 0\n",
    "# mask3 = df1[to_check].sum(axis=1) == 1\n",
    "# print(len(df1.loc[mask, 'no_finding_major'])+len(df1.loc[mask2, 'no_finding_major'])+len(df1.loc[mask3, 'no_finding_major'])) #15000 outputted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c63642c2-9c68-41fa-ab66-cc6139be3817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "mask1 = df1[to_check].sum(axis=1) == 0\n",
    "mask2 = df1['no_finding_major'] == 1\n",
    "print(mask1[mask2].all()) #proving the important reverse statement, no_finding_major == 1 => sum==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5ccd00ee-d5bd-4322-a35e-aab922840592",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\"/workspace/medAI-eval-pipeline/dataset_artifacts/bbox.csv\")\n",
    "\n",
    "#sanity check\n",
    "cols_blank = ['x_min', 'x_max', 'y_min', 'y_max']\n",
    "mask = df2['class_id'] == 14\n",
    "assert df2.loc[mask, cols_blank].isna().all().all(), \\\n",
    "       \"there exists erronous cases where patients who are healthy have boxes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f4b3da46-3b51-481b-a56f-f04596367b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add relative image path to classification.csv\n",
    "csf_df = pd.read_csv(\"/workspace/medAI-eval-pipeline/dataset_artifacts/classification.csv\")\n",
    "csf_df['rel_image_path'] = \"/workspace/medAI-eval-pipeline/256data/train/\" + csf_df['image_id'].astype(str) + \".png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1811ff41-4d45-4db5-99e9-6861c77c3a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/medAI-eval-pipeline/256data/train/000434271f63a053c4128a0ba6352c7f.png'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csf_df['rel_image_path'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a1e91754-6506-4ee6-822a-6435e5fe97c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "csf_df.insert(0, 'rel_image_path', csf_df.pop('rel_image_path'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b14a612f-faf8-4c55-bf22-e54b7261c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "csf_df.to_csv(os.path.join(OUT_DIR, \"classification_withimgpath.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a43e65b3-e604-426c-bf43-1579cad56d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['image_id'].nunique() == len(csf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d935e7-2ac0-415d-8bc7-c5051f8fcbd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
